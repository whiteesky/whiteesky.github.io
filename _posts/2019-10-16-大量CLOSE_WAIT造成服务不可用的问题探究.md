---
layout: post
title: 大量CLOSE_WAIT造成服务不可用的问题探究
categories: 技术总结
author: whiteesky
date: 2019-10-16 21:26:32
tags:
  - Java
  - JVM
  - TCP
header-img: img/article/close_wait/header.jpg
copyright: true
---
### 问题背景

线上某关键服务突然响应极慢，严重不可用。
发现服务部分节点出现问题：
![1]

有一些奇怪的是，服务依然存在完好的节点，但是流量并未转到完好的节点上，还是打在有问题的节点上，导致服务严重不可用。

当时习惯性的甩锅给了平台团队，先重启节点暂时解决了线上问题，但数天后会再次出现问题。这时真的觉得可能是自己的问题了。

### 问题排查

#### 文件IO未关闭？

先观察 `grafana` 监控，在服务有问题的时刻，文件描述符数量处于明显飙升态势：
![2]

首先，怀疑 `文件IO` 操作是否存在未关闭状态，过了一遍该服务的代码，并不存在问题。

#### 网络连接问题？

接下来，怀疑是否存在网络连接的问题。在下次出问题时，重启前先通过 `netstat -ano` 保存了节点的网络连接状态现场：
![3]

当时出现了大量（数百个）的处于 `CLOSE_WAIT` 状态的TCP链接；`172.19.32.75:9900` 正是该服务。

`CLOSE_WAIT` 状态出现在 `TCP` 四次挥手的过程中：
![4]

可以看到，若服务A主动发起连接关闭，服务B收到了A发送的 `FIN` 包，会回复A `ACK` 并转换到 `CLOSE_WAIT` 状态。接下来B全部处理完自己的逻辑之后发出 `FIN` 包给A，并转向 `LAST_ACK`。如果服务出现大量请求停止在 `CLOSE_WAIT` 状态，那么说明服务收到了关闭的请求，但是自己最终没有发出 `FIN` 包。

##### HttpClient使用不当？

常见的一种原因是服务通过 `HttpClient` 进行请求，代码不够完善。导致当出现该服务被动关闭连接情况时，未能正确释放连接，使得状态停留在 `CLOSE_WAIT`。通过review代码，发现并没有手动使用原生 `HttpClient` 进行请求。

##### 前端请求超时？

观察服务接口的响应时间，发现普遍很长，很大一部分原因是因为刚刚上线了一个同步等待银行支付的接口：
![5]

此时怀疑是否在一段时间内，很多请求的响应时间超过了前端的超时时间，导致前端关闭连接，服务端在处理时一直还停留在 `CLOSE_WAIT` 状态。询问前端，并排查后端的上游服务，发现超时时间均较长，排除此原因。

##### Tomcat维持长连接问题？

接下来，怀疑是否在使用 `keep-alive` 的情况下，前端虽然不超时，但用户在请求中关闭页面终止连接，但此时前端发出的 `FIN` 包会被 `Tomcat` 屏蔽。而 `Tomcat` 只会在 `keep-alive` 时间到且逻辑执行完时关闭连接？

为了验证这个问题，写一个测试接口 `sleep 30s` 进行测试：
![6]

通过浏览器发送请求，观察请求头，确实使用了 `keep-alive`：
![7]

首先正常等待接口成功返回后，通过 `netstat` 观察，此时连接确实还处在 `ESTABLISHED`，证明是保持了长连接状态。

接下来实验异常情况。通过浏览器发送请求，接着快速关闭浏览器，模拟异常关闭的操作。此时接口逻辑还未执行完，通过 `netstat` 观察，确实连接处于 `CLOSE_WAIT` 状态：
![8]

等待30s接口执行完后，再次通过 `netstat` 观察，发现连接已经消失。说明此前的推测是错误的，`Tomcat` 应该处理了异常关闭的情形。

### 最终解决

排查到现在，已经完全处于懵圈的状态。

此刻再看当时的问题现场，才发现 `Foreign Address` 竟然看起来不像是 `Nginx`，而是和 `Local Address` 一样都是 `172.19.32.xx`。

其实还有一个在日志里没有体现的请求，就是平台宿主机的 `Health Check` 请求。

后来再重看文件描述符数量图，才想到增长速率有些过于稳定，其实这是很诡异的事情。

平台组表示，宿主机 `Health Check` 配置如下：
```yaml
"timeoutSeconds": 3,
"periodSeconds": 30,
"successThreshold": 1,
"failureThreshold": 3
```

可以看到 `Health Check` 30秒/次，而从文件描述符的监控可以看出，增长的速率是基本匹配的，终于找到了原因。

服务本身堆内存给的是有些偏少的，只有1G。从节点的内存状态去看，当时的内存占用已经达到一个很高的水平，但又还没有完全达到到被 `OOM Killed` 的程度。

因此可以推断，服务接口响应本身响应偏慢，当时内存过高会导致请求得不到及时响应，连接池打满，使得 `Health Check` 不能被及时响应。而 `Health Check` 超时时间是3秒，超时后则主动关闭了连接，导致服务端产生 `CLOSE_WAIT`，并且一直未释放。

为何高可用未起作用？推测是偶尔 `Health Check` 会成功1次，配置中只要成功就认为节点完好，所以可能节点的状态一直处于时好时坏的情况，没有被彻底摘除，使得请求还是经常会被打到不可用的节点上。

### 总结

通过排查，其实根本的原因还是在内存占用上，并不是一个特别典型的因为代码的不完善而导致大量 `CLOSE_WAIT` 的问题。

最终的解决方案，先是增大了内存，毕竟之前内存设置确实有点儿小了。而对于支付接口其实也可以进行一下产品逻辑上的优化，不一定要死等银行的结果。

但实际来说，这个问题比大家常见的内存问题引起的 `OOM` 要危险许多。一般的平台遇到 `OOM` 都会帮你重启，过程中高可用也肯定会起作用，造成不了什么影响。但是本文这种情况，直接使得高可用不起作用，并且如果没有发现一直任其发展下去，雪崩效应也会把其他好的节点带坏。

总体来说，出现大量 `CLOSE_WAIT` 的问题，基本思路其实就是，对方主动关闭了，但是你最终没有关闭。按照这个原因去一点一点找就可以了。



[1]: /img/article/close_wait/1.png
[2]: /img/article/close_wait/2.png
[3]: /img/article/close_wait/3.png
[4]: /img/article/close_wait/4.png
[5]: /img/article/close_wait/5.png
[6]: /img/article/close_wait/6.png
[7]: /img/article/close_wait/7.png
[8]: /img/article/close_wait/8.png
