---
layout: post
title: 摸石头过河：第一次JVM实战经验总结
categories: 技术总结
author: whiteesky
date: 2018-04-13 20:31:12
tags:
  - Java
  - JVM
  - GC
header-img: img/article/jvm_gc/header.jpg
copyright: true
---
之前虽然储备了一些JVM相关理论的知识，但是似乎一直没有什么机会到真正的业务中进行实战。

凡事都不禁念叨，跟别人念叨了一句我们的服务从来不需要调优，果然服务就开始抽风了。不过也正好有机会实践一下之前学习的东西。

疯狂的Full GC
---
我们的服务处在转型微服务的初期，服务运行已经比较稳定，各项监控、运维等工具也在陆续搭建过程中。有天在打开某服务的PinPoint监控时，虎躯一震，竟然每隔2分钟一次Full GC：

![1]

都知道Full GC会Stop-The-World，两分钟一次还能玩么。。。。

不过由于业务特殊性，本身也没有那么大的并发量，实际上并没有想象的特别影响使用，也由于还没有完善的监控报警措施，所以一直没有发现这个问题。但其实问题也已经持续了好几天。

线上找个节点先利用JDK自带的JVM监控命令看一下：
```
jps -l
```
jps可以显示系统内所有的虚拟机进程。-l参数可以看到主类或Jar的全名。

```
jstat -gc 1 5000
```
jstat命令是用于监视虚拟机运行时状态信息的命令。-gc参数可以看到垃圾回收堆的行为统计；1是用jps得到的进程号；5000代表每隔5000ms会再次输出当前的状态。

下图是上述jstat命令打印出的状态信息（示例图，非真实数据）：
![2]

排查问题时，我们发现OU（老年代已使用的容量）在疯涨，每两分钟左右就会涨满至OC（老年代总容量），也就会触发Full GC。

此时决定查看一下是否存在大对象：
```
jmap -histo 1 | head
```
jmap命令用于生成堆dump文件。-histo参数能够显示堆中对象的统计信息。如下所示（示例图，非真实数据）：
![3]

当时的堆dump信息类似上图，并没有一个明显不常见的大对象出现，也就char[]显得多了些。我们利用JDK自带的JVisualVM来分析dump文件，char[]的引用情况也没有发现明显不正常。

结果明显偏离了我们的预期。当时的思路总是卡在大对象上，总觉得这种情况就是大对象的问题，这下思路一下中断了。

于是当时我们先用了一个办法，本地起服务，利用压力测试的方式挨个刷接口，看下有没有问题。

我们使用Java的压力测试工具[Apache JMeter][4]来模拟线上场景。下图在测试计划中建立了一个线程组无限循环，内部是一个HTTP请求打到不同的接口上。
![5]

![6]

我们利用JVisualVM监控本地的服务。终于，当刷到一个验证码接口时，出现了老年代会比较快上涨的情况。此时导出堆Dump信息（示例图，非真实数据）：
![7]

出现了一个明显不正常的对象AfflieTransform。大概看了下相关源码，我们推测，由于生成图形验证码时，可以对上面的文字图像进行旋转。而每个记录了不同角度图形变换的AfflieTransform对象可能都会先被缓存起来。

原本的代码中，旋转角度是一个随机的double值，这样每次的角度几乎都不同。我们尝试将代码改为随机一定范围内的整数值，再次刷验证码接口，果然老年代快速上涨的情况被遏制住了。

不过问题并没有这么简单被解决，当时忘记了一点，线上这个接口的qps真的有像JMeter压测接口时那么大吗？

问题依然严重
---
抛开剂量谈毒性，果然都是耍流氓。
调整验证码接口重新上线后，似乎Full GC的频率稍稍慢了一丢丢丢丢。问题依然还是很严重。

这下终于反应过来了，又不一定非得是代码写出内存泄漏了导致了Full GC，访问压力过大，不是也可以？
此时就显出了监控缺失的问题，对于接口qps并没有监控，无法及时发现问题。

赶紧去日志平台上统计接口访问量的数据，终于发现了某个接口的访问量在某天突然非正常暴增。
再统计下访问该接口的IP地址，明显有一批重复的IP。好吧，看来是被爬了。

抓紧加一段封非正常IP的逻辑，再上线，观察下，果然，Full GC看来是消失了。

还有其他问题
---
但是，问题并没有完全被解决。第二天再观察监控，发现Full GC其实并没有完全消失，而是隔两个小时左右会有一次：
![8]

当然，比起之前两分钟一次的Full GC，已经非常好了。但是，两小时一次Full GC其实依然不算太正常。服务压力大的问题已经不存在了，到底还有什么问题呢？

再看下jstat命令打印出的状态信息（模拟示例图，非真实数据）：
![9]

通过连续打印出的状态信息能够清楚看到Eden区满时Minor GC的情况、Survivor区的交换、Old区的增长…………等等等等，Survivor区和Eden区的大小比例为什么差着好几十倍？？说好的默认8:1呢？？

网上查了下，也有人出现过同样的情况，罪魁祸首竟然是：并行系的收集器会默认开启-XX:+UseAdaptiveSizePolicy， 它会在每次Minor GC之后对年轻代进行自适应分配大小。所以引起了Survivor区和Eden区比例明显失调。这点其实不太明白具体的适应算法是什么样的，为什么会出现这种明显不合适的情况发生。

所以解决方法就是先把该参数关掉：-XX:-UseAdaptiveSizePolicy （+为开启参数，-为关闭参数）。

再将-XX:SurvivorRatio设置好需要的比例即可。比如-XX:SurvivorRatio=8就意味着Eden区与两个Survivor区的比值为8:1:1。

问题终于全部解决：
![10]

经验总结
---
一个新的服务上线，尤其是微服务架构，监控和告警一定要尽可能马上跟上。像文中这种虽然服务跑在公司云平台上能够接到服务OOM等报警，但是接口qps出现明显异常的情况，影响性能但服务没有挂，我们就不能接到告警了，找问题时也绕了个大圈子。当时如果对重要的接口有监控，那么不仅能够尽早发现问题，也能瞬间定位到问题所在。

至于解决此类问题的能力，我觉得基本还是一个实战经验累积的过程。理论看了半天，最后也还是要具体问题具体分析，一个一个可能性去挨个排查。就像校招时候都会背8:1:1，结果根本没想到它还会自动变。这次也是通过这个问题，真正实践了一下各种命令和工具的使用，也利用原来学过的一些理论指导我们自己排查问题。过程中使用的方法和思路也许并不是太好，不过最终结果还是好的，问题解决，也收获了经验。


  [1]: /img/article/jvm_gc/1.jpeg
  [2]: /img/article/jvm_gc/2.png
  [3]: /img/article/jvm_gc/3.png
  [4]: https://jmeter.apache.org/
  [5]: /img/article/jvm_gc/4.png
  [6]: /img/article/jvm_gc/5.png
  [7]: /img/article/jvm_gc/6.png
  [8]: /img/article/jvm_gc/7.jpeg
  [9]: /img/article/jvm_gc/2.png
  [10]: /img/article/jvm_gc/8.jpeg
